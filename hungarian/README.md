From the paper "A highly accurate Named Entity corpus for Hungarian"

Citation:
Szarvas, G., Farkas, R., Felfoldi, L., Kocsor, A., & Csirik, J. (2006, May). A highly accurate Named Entity corpus for Hungarian. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LRECâ€™06).

BibTex:
@inproceedings{szarvas-etal-2006-highly,
    title = "A highly accurate Named Entity corpus for {H}ungarian",
    author = {Szarvas, Gy{\"o}rgy  and
      Farkas, Rich{\'a}rd  and
      Felf{\"o}ldi, L{\'a}szl{\'o}  and
      Kocsor, Andr{\'a}s  and
      Csirik, J{\'a}nos},
    booktitle = "Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)",
    month = may,
    year = "2006",
    address = "Genoa, Italy",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2006/pdf/365_pdf.pdf",
    abstract = "A highly accurate Named Entity (NE) corpus for Hungarian that is publicly available for research purposes is introduced in the paper, along with its main properties. The results of experiments that apply various Machine Learning models and classifier combination schemes are also presented to serve as a benchmark for further research based on the corpus. The data is a segment of the Szeged Corpus (Csendes et al., 2004), consisting of short business news articles collected from MTI (Hungarian News Agency, www.mti.hu). The annotation procedure was carried out paying special attention to annotation accuracy. The corpus went through a parallel annotation phase done by two annotators, resulting in a tagging with inter-annotator agreement rate of 99.89{\%}. Controversial taggings were collected and discussed by the two annotators and a linguist with several years of experience in corpus annotation. These examples were tagged following the decision they made together, and finally all entities that had suspicious or dubious annotations were collected and checked for consistency. We consider the result of this correcting process virtually be free of errors. Our best performing Named Entity Recognizer (NER) model attained an accuracy of 92.86{\%} F measure on the corpus.",
}

Dataset Link:
https://rgai.inf.u-szeged.hu/node/130

Notes:
IOB encoding